# Deep Research Agent - Web Implementation

ğŸ§  **AI ê¸°ë°˜ ì‹¬ì¸µ ì—°êµ¬ ì—ì´ì „íŠ¸** / AI-Powered Deep Research Agent

Ollamaë¥¼ í™œìš©í•œ ë¡œì»¬ LLM ê¸°ë°˜ ì›¹ ì—°êµ¬ ë„êµ¬ë¡œ, í•œêµ­ì–´ì™€ ì˜ì–´ë¥¼ ì§€ì›í•©ë‹ˆë‹¤.  
A web-based research tool powered by local LLM via Ollama, supporting Korean and English.

## âœ¨ ì£¼ìš” íŠ¹ì§• / Key Features

- ğŸ¤– **ë¡œì»¬ LLM í†µí•©** - Ollamaì™€ Gemma 3 ëª¨ë¸ì„ ì‚¬ìš©í•œ í”„ë¼ì´ë²„ì‹œ ë³´ì¥
- ğŸŒ **ë‹¤êµ­ì–´ ì§€ì›** - í•œêµ­ì–´/ì˜ì–´ ìë™ ê°ì§€ ë° ì‘ë‹µ
- âš¡ **ì‹¤ì‹œê°„ ì§„í–‰ ìƒí™©** - WebSocket ê¸°ë°˜ ì‹¤ì‹œê°„ ì—…ë°ì´íŠ¸
- ğŸ“Š **ê³ í’ˆì§ˆ ë³´ê³ ì„œ** - ë‹¤ì¤‘ ì†ŒìŠ¤ ê¸°ë°˜ êµ¬ì¡°í™”ëœ ì—°êµ¬ ë³´ê³ ì„œ
- ğŸ³ **Docker ì§€ì›** - ê°„í¸í•œ ë°°í¬ ë° ê´€ë¦¬

## ğŸ—ï¸ ì•„í‚¤í…ì²˜ / Architecture

```
Frontend (Next.js) â†” Backend (FastAPI) â†” Ollama (Gemma 3:12B)
                            â†“
                    SQLite Database
```

- **Frontend**: Next.js 14, TypeScript, Tailwind CSS
- **Backend**: FastAPI, LangGraph, Pydantic
- **LLM**: Ollama with Gemma 3:12B
- **Database**: SQLite (PostgreSQL ready)
- **Search**: Tavily API

ğŸ“Š **[ì‹œìŠ¤í…œ êµ¬ì„±ë„ ë³´ê¸° / View System Architecture](./ARCHITECTURE.md)**

## ğŸš€ ë¹ ë¥¸ ì‹œì‘ / Quick Start

### ì „ì œ ì¡°ê±´ / Prerequisites

1. **Docker & Docker Compose** (ê¶Œì¥ / Recommended)
2. **Ollama** (ë¡œì»¬ ì„¤ì¹˜ ì‹œ / For local installation)
3. **Node.js 18+** (ê°œë°œ ì‹œ / For development)
4. **Python 3.11+** (ê°œë°œ ì‹œ / For development)

### Dockerë¡œ ì‹œì‘í•˜ê¸° / Getting Started with Docker

1. **ì €ì¥ì†Œ ë³µì œ / Clone Repository**
```bash
git clone <repository-url>
cd open_deep_research
```

2. **í™˜ê²½ ì„¤ì • / Environment Setup**
```bash
cp .env.example .env
# Edit .env file with your settings
```

3. **ì„œë¹„ìŠ¤ ì‹œì‘ / Start Services**
```bash
docker-compose up -d
```

4. **ë¸Œë¼ìš°ì €ì—ì„œ ì ‘ì† / Access in Browser**
- Frontend: http://localhost:3000
- Backend API: http://localhost:8000
- API Docs: http://localhost:8000/docs

### ë¡œì»¬ ê°œë°œ ì„¤ì • / Local Development Setup

#### 1. Ollama ì„¤ì¹˜ ë° ì„¤ì • / Ollama Installation

```bash
# Ollama ì„¤ì¹˜ / Install Ollama
curl -fsSL https://ollama.ai/install.sh | sh

# Ollama ì„œë¹„ìŠ¤ ì‹œì‘ / Start Ollama service
ollama serve

# Gemma 3 ëª¨ë¸ ë‹¤ìš´ë¡œë“œ / Download Gemma 3 model
ollama pull gemma3:12b
```

#### 2. ë°±ì—”ë“œ ì„¤ì • / Backend Setup

```bash
cd backend

# Poetry ì„¤ì¹˜ (if not installed)
pip install poetry

# ì˜ì¡´ì„± ì„¤ì¹˜ / Install dependencies
poetry install

# í™˜ê²½ ë³€ìˆ˜ ì„¤ì • / Setup environment
cp .env.example .env

# ê°œë°œ ì„œë²„ ì‹œì‘ / Start development server
poetry run uvicorn src.open_deep_research.api.main:socket_app --reload --host 0.0.0.0 --port 8000
```

#### 3. í”„ë¡ íŠ¸ì—”ë“œ ì„¤ì • / Frontend Setup

```bash
cd frontend

# ì˜ì¡´ì„± ì„¤ì¹˜ / Install dependencies
npm install

# ê°œë°œ ì„œë²„ ì‹œì‘ / Start development server
npm run dev
```

## ğŸ”§ í™˜ê²½ ì„¤ì • / Configuration

### í•„ìˆ˜ í™˜ê²½ ë³€ìˆ˜ / Required Environment Variables

```env
# Ollama ì„¤ì •
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL=gemma3:12b

# ê²€ìƒ‰ API (ì„ íƒì‚¬í•­)
TAVILY_API_KEY=your_tavily_api_key

# ë°ì´í„°ë² ì´ìŠ¤
DATABASE_URL=postgresql+asyncpg://user:pass@localhost:5432/research_db
```

### Tavily API í‚¤ ì„¤ì • / Tavily API Key Setup

1. [Tavily](https://app.tavily.com/)ì—ì„œ ê³„ì • ìƒì„±
2. API í‚¤ ë°œê¸‰
3. `.env` íŒŒì¼ì— `TAVILY_API_KEY` ì„¤ì •

> **ì°¸ê³ **: Tavily API í‚¤ê°€ ì—†ì–´ë„ ì‹œìŠ¤í…œì€ ì‘ë™í•˜ì§€ë§Œ, ê²€ìƒ‰ ê¸°ëŠ¥ì´ ì œí•œë©ë‹ˆë‹¤.

## ğŸ“– ì‚¬ìš© ë°©ë²• / How to Use

### 1. ê¸°ë³¸ ì—°êµ¬ ì‹¤í–‰ / Basic Research

1. ì›¹ ì¸í„°í˜ì´ìŠ¤ ì ‘ì† (http://localhost:3000)
2. ì—°êµ¬ ì§ˆë¬¸ ì…ë ¥ (í•œêµ­ì–´ ë˜ëŠ” ì˜ì–´)
3. **ì—°êµ¬ ì‹œì‘** ë²„íŠ¼ í´ë¦­
4. ì‹¤ì‹œê°„ ì§„í–‰ ìƒí™© ëª¨ë‹ˆí„°ë§
5. ì™„ë£Œ í›„ ê²°ê³¼ í™•ì¸ ë° ë‹¤ìš´ë¡œë“œ

### 2. ê³ ê¸‰ ì„¤ì • / Advanced Settings

- **ì–¸ì–´**: ìë™ ê°ì§€, í•œêµ­ì–´, ì˜ì–´
- **ì—°êµ¬ ê¹Šì´**: ê°„ë‹¨, ë³´í†µ, ì‹¬ì¸µ
- **ë™ì‹œ ì—°êµ¬ì› ìˆ˜**: 1-5ëª… (ë³‘ë ¬ ì²˜ë¦¬)

### 3. API ì‚¬ìš©ë²• / API Usage

```bash
# ì—°êµ¬ ì‹œì‘ / Start Research
curl -X POST "http://localhost:8000/api/v1/research/start" \
  -H "Content-Type: application/json" \
  -d '{"query": "AI ê¸°ìˆ ì˜ ìµœì‹  ë™í–¥", "language": "ko"}'

# ì—°êµ¬ ìƒíƒœ í™•ì¸ / Check Research Status
curl "http://localhost:8000/api/v1/research/{session_id}"

# ë³´ê³ ì„œ ì¡°íšŒ / Get Report
curl "http://localhost:8000/api/v1/research/{session_id}/report"
```

## ğŸ› ï¸ ê°œë°œ / Development

### í”„ë¡œì íŠ¸ êµ¬ì¡° / Project Structure

```
open_deep_research/
â”œâ”€â”€ backend/                 # FastAPI ë°±ì—”ë“œ
â”‚   â”œâ”€â”€ src/open_deep_research/
â”‚   â”‚   â”œâ”€â”€ api/            # API ì—”ë“œí¬ì¸íŠ¸
â”‚   â”‚   â”œâ”€â”€ core/           # í•µì‹¬ ë¡œì§
â”‚   â”‚   â”œâ”€â”€ models/         # ë°ì´í„° ëª¨ë¸
â”‚   â”‚   â”œâ”€â”€ services/       # ì„œë¹„ìŠ¤ ë ˆì´ì–´
â”‚   â”‚   â”œâ”€â”€ prompts/        # í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
â”‚   â”‚   â””â”€â”€ utils/          # ìœ í‹¸ë¦¬í‹°
â”‚   â”œâ”€â”€ tests/              # í…ŒìŠ¤íŠ¸
â”‚   â””â”€â”€ pyproject.toml      # ì˜ì¡´ì„± ê´€ë¦¬
â”œâ”€â”€ frontend/               # Next.js í”„ë¡ íŠ¸ì—”ë“œ
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ app/           # Next.js App Router
â”‚   â”‚   â”œâ”€â”€ components/    # React ì»´í¬ë„ŒíŠ¸
â”‚   â”‚   â”œâ”€â”€ hooks/         # React í›…
â”‚   â”‚   â””â”€â”€ lib/           # ë¼ì´ë¸ŒëŸ¬ë¦¬
â”‚   â””â”€â”€ package.json       # ì˜ì¡´ì„± ê´€ë¦¬
â””â”€â”€ docker-compose.yml     # Docker êµ¬ì„±
```

### í…ŒìŠ¤íŠ¸ ì‹¤í–‰ / Running Tests

```bash
# ë°±ì—”ë“œ í…ŒìŠ¤íŠ¸ / Backend Tests
cd backend
poetry run pytest

# í”„ë¡ íŠ¸ì—”ë“œ í…ŒìŠ¤íŠ¸ / Frontend Tests
cd frontend
npm run test
```

### ì½”ë“œ í’ˆì§ˆ / Code Quality

```bash
# ë°±ì—”ë“œ ë¦°íŒ… / Backend Linting
cd backend
poetry run ruff check
poetry run mypy src

# í”„ë¡ íŠ¸ì—”ë“œ ë¦°íŒ… / Frontend Linting
cd frontend
npm run lint
npm run type-check
```

## ğŸ” íŠ¸ëŸ¬ë¸”ìŠˆíŒ… / Troubleshooting

### ì¼ë°˜ì ì¸ ë¬¸ì œ / Common Issues

1. **Ollama ì—°ê²° ì‹¤íŒ¨**
   - Ollama ì„œë¹„ìŠ¤ê°€ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸: `ollama list`
   - ëª¨ë¸ì´ ì„¤ì¹˜ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸: `ollama list`
   - í¬íŠ¸ ì¶©ëŒ í™•ì¸: `netstat -tulpn | grep 11434`

2. **ê²€ìƒ‰ ê¸°ëŠ¥ ì œí•œ**
   - Tavily API í‚¤ê°€ ì„¤ì •ë˜ì—ˆëŠ”ì§€ í™•ì¸
   - API í‚¤ì˜ ìœ íš¨ì„± í™•ì¸

3. **WebSocket ì—°ê²° ì˜¤ë¥˜**
   - CORS ì„¤ì • í™•ì¸
   - ë°©í™”ë²½ ì„¤ì • í™•ì¸
   - ë¸Œë¼ìš°ì € ì½˜ì†”ì—ì„œ ì˜¤ë¥˜ ë©”ì‹œì§€ í™•ì¸

### ë¡œê·¸ í™•ì¸ / Log Checking

```bash
# Docker ë¡œê·¸ í™•ì¸ / Check Docker Logs
docker-compose logs -f backend
docker-compose logs -f frontend
docker-compose logs -f ollama

# ë°±ì—”ë“œ ì§ì ‘ ì‹¤í–‰ ì‹œ ë¡œê·¸ / Direct Backend Logs
LOG_LEVEL=DEBUG poetry run uvicorn src.open_deep_research.api.main:socket_app --reload
```

## ğŸ“Š ì„±ëŠ¥ ìµœì í™” / Performance Optimization

### ì‹œìŠ¤í…œ ìš”êµ¬ì‚¬í•­ / System Requirements

- **ìµœì†Œ**: 8GB RAM, 2 CPU cores
- **ê¶Œì¥**: 16GB RAM, 4+ CPU cores
- **ë””ìŠ¤í¬**: 10GB+ (ëª¨ë¸ ì €ì¥ìš©)

### ì„±ëŠ¥ íŠœë‹ / Performance Tuning

1. **Ollama ì„¤ì •**
   ```bash
   # GPU ì‚¬ìš© (NVIDIA CUDA)
   CUDA_VISIBLE_DEVICES=0 ollama serve
   
   # ë©”ëª¨ë¦¬ ì œí•œ
   OLLAMA_HOST=0.0.0.0:11434 OLLAMA_MAX_LOADED_MODELS=1 ollama serve
   ```

2. **ë™ì‹œ ì—°êµ¬ì› ìˆ˜ ì¡°ì ˆ**
   - ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤ì— ë”°ë¼ 1-5ëª… ì¡°ì ˆ
   - CPU ì½”ì–´ ìˆ˜ì— ë”°ë¼ ìµœì í™”

## ğŸ“ ë¼ì´ì„¼ìŠ¤ / License

MIT License - ìì„¸í•œ ë‚´ìš©ì€ LICENSE íŒŒì¼ì„ ì°¸ì¡°í•˜ì„¸ìš”.

## ğŸ¤ ê¸°ì—¬ / Contributing

1. Fork the repository
2. Create a feature branch: `git checkout -b feature/amazing-feature`
3. Commit your changes: `git commit -m 'Add amazing feature'`
4. Push to the branch: `git push origin feature/amazing-feature`
5. Open a Pull Request

## ğŸ“ ì§€ì› / Support

- ì´ìŠˆ ë¦¬í¬íŠ¸: [GitHub Issues](https://github.com/your-repo/issues)
- ë¬¸ì„œ: [Architecture Design](./architecture_design.md)
- ë°ì´í„° íë¦„: [Data Flow Analysis](./data_flow_analysis.md)

---

**Based on**: [Open Deep Research](https://github.com/langchain-ai/open_deep_research) by LangChain AI